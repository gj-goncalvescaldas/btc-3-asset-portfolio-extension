{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ce50a8-0138-4983-8a4c-5609ea1c1330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RMSE (erro quadr√°tico m√©dio): 3679.65\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mC:\\anaconda\\envs\\pyspark_env1\\lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: -1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Estimar o comportamento (crescimento simples de 2% por ano)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m365\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     future_df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose_btc_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfuture_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose_btc_target\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.02\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Converter o DataFrame futuro para Spark\u001b[39;00m\n\u001b[0;32m     75\u001b[0m future_df_spark \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(future_df)\n",
      "File \u001b[1;32mC:\\anaconda\\envs\\pyspark_env1\\lib\\site-packages\\pandas\\core\\indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\envs\\pyspark_env1\\lib\\site-packages\\pandas\\core\\frame.py:4221\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 4221\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   4224\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   4225\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\envs\\pyspark_env1\\lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# üìò Notebook: Previs√£o do Pre√ßo da Bitcoin (At√© 2029)\n",
    "\n",
    "# ‚úÖ 1. Carregar dados\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"../data/final_btc_ml_dataset.csv\")\n",
    "df = df.withColumn(\"Date\", to_date(\"Date\"))\n",
    "\n",
    "# ‚úÖ 2. Filtrar e ordenar os dados para previs√£o\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lead\n",
    "\n",
    "df = df.orderBy(\"Date\")\n",
    "df = df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Date\")))\n",
    "\n",
    "# ‚úÖ 3. Criar vari√°vel target futura (previs√£o de 365 dias ou mais)\n",
    "df = df.withColumn(\"close_btc_target\", lead(\"close_btc\", 365).over(Window.orderBy(\"Date\")))\n",
    "df = df.dropna(subset=[\"close_btc_target\"])\n",
    "\n",
    "# ‚úÖ 4. Selecionar features relevantes\n",
    "features = [\n",
    "    'open_btc', 'high_btc', 'low_btc', 'close_btc', 'volume_btc', 'block_reward', 'cpi',\n",
    "    'open_sp500', 'high_sp500', 'low_sp500', 'close_sp500', 'volume_sp500',\n",
    "    'SMA_20_btc', 'BB_std_btc', 'BB_upper_btc', 'BB_lower_btc', 'RSI_btc',\n",
    "    'EMA_12_btc', 'EMA_26_btc', 'MACD_btc', 'MACD_signal_btc',\n",
    "    'USDCHF', 'EURCHF'\n",
    "]\n",
    "\n",
    "# ‚úÖ 5. Remover linhas com null nas features ou na target\n",
    "df = df.dropna(subset=features + [\"close_btc_target\"])\n",
    "\n",
    "# ‚úÖ 6. Preparar dados para modelagem\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"Date\", \"features\", \"close_btc_target\")\n",
    "\n",
    "# ‚úÖ 7. Dividir treino/teste\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ‚úÖ 8. Treinar modelo\n",
    "rf = RandomForestRegressor(labelCol=\"close_btc_target\", featuresCol=\"features\", numTrees=100)\n",
    "model = rf.fit(train)\n",
    "\n",
    "# ‚úÖ 9. Avaliar\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"close_btc_target\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"‚úÖ RMSE (erro quadr√°tico m√©dio): {rmse:.2f}\")\n",
    "\n",
    "# ‚úÖ 10. Visualizar previs√µes\n",
    "pdf_1_year = predictions.select(\"Date\", \"close_btc_target\", \"prediction\").toPandas().sort_values(\"Date\")\n",
    "\n",
    "# ‚úÖ 11. Estender a previs√£o at√© 2029\n",
    "# Obter a √∫ltima data do DataFrame `df_combined`\n",
    "last_date = df.select(\"Date\").toPandas().tail(1)['Date'].values[0]\n",
    "\n",
    "# Criar datas futuras (2025-2029)\n",
    "extended_dates = pd.date_range(last_date, periods=5*365, freq='D')\n",
    "\n",
    "# Criar um DataFrame com as datas estendidas\n",
    "future_df = pd.DataFrame({'Date': extended_dates})\n",
    "future_df['close_btc_target'] = None  # Placeholder para a previs√£o\n",
    "\n",
    "# Estimar o comportamento (crescimento simples de 2% por ano)\n",
    "for i in range(365 * 5):\n",
    "    future_df.loc[i, 'close_btc_target'] = future_df.loc[i-1, 'close_btc_target'] * (1 + 0.02)\n",
    "\n",
    "# Converter o DataFrame futuro para Spark\n",
    "future_df_spark = spark.createDataFrame(future_df)\n",
    "df_combined_extended = df_combined.join(future_df_spark, on=\"Date\", how=\"left\")\n",
    "\n",
    "# ‚úÖ 12. Visualizar as previs√µes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pdf_1_year[\"Date\"], pdf_1_year[\"close_btc_target\"], label=\"Real\", linewidth=2)\n",
    "plt.plot(pdf_1_year[\"Date\"], pdf_1_year[\"prediction\"], label=\"Previsto (1 ano)\", linestyle=\"--\")\n",
    "plt.plot(future_df[\"Date\"], future_df[\"close_btc_target\"], label=\"Previs√£o at√© 2029\", linestyle=\":\")\n",
    "plt.title(\"Previs√£o do Pre√ßo da Bitcoin (1 ano √† frente e at√© 2029)\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Pre√ßo BTC (USD)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533597e-e852-4f11-bb09-35cd9a2aba7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
