{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Users\\TESTER\\anaconda3\\envs\\btc_portfolio\\Library\n",
      "SPARK_LOCAL_DIRS: C:\\Users\\TESTER\\Desktop\\Laboral\\GIT\\btc-3-asset-portfolio-extension\\notebooks\\spark-temp\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta temporal Spark\n",
    "temp_path = os.path.join(os.getcwd(), 'spark-temp')\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "\n",
    "# Definir variables de entorno\n",
    "os.environ['JAVA_HOME'] = os.environ['CONDA_PREFIX'] + '\\Library'\n",
    "os.environ['SPARK_LOCAL_DIRS'] = temp_path\n",
    "\n",
    "print('JAVA_HOME:', os.environ.get('JAVA_HOME'))\n",
    "print('SPARK_LOCAL_DIRS:', os.environ.get('SPARK_LOCAL_DIRS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.4\n",
      "Tiempo total en crear SparkSession: 0.01 segundos\n"
     ]
    }
   ],
   "source": [
    "# Crear Spark Session y medir tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('btcproject') \\\n",
    ".config('spark.driver.memory', '8g') \\\n",
    ".config('spark.executor.memory', '8g') \\\n",
    ".config('spark.local.dir', temp_path) \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "end_time = time.time()\n",
    "print('Spark Version:', spark.version)\n",
    "print(f'Tiempo total en crear SparkSession: {round(end_time - start_time, 2)} segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Btc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = spark.read \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".csv('../data/Bitcoin Historical Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+--------+-----+--------+\n",
      "|      Date|    Price|     Open|     High|     Low| Vol.|Change %|\n",
      "+----------+---------+---------+---------+--------+-----+--------+\n",
      "|04/01/2025| 84,931.6| 82,548.6| 88,377.3|74,524.2|1.29M|   2.89%|\n",
      "|03/01/2025| 82,548.8| 84,353.4| 94,986.5|76,677.1|2.37M|  -2.17%|\n",
      "|02/01/2025| 84,381.2|102,421.3|102,770.8|78,329.8|2.40M| -17.62%|\n",
      "|01/01/2025|102,424.2| 93,557.3|109,228.6|89,664.8|2.72M|   9.48%|\n",
      "|12/01/2024| 93,557.2| 96,404.7|108,244.9|91,522.3|4.41M|  -2.95%|\n",
      "|11/01/2024| 96,405.7| 70,278.7| 99,617.4|66,834.0|4.16M|  37.17%|\n",
      "|10/01/2024| 70,281.8| 63,329.9| 73,569.4|59,075.7|2.56M|  10.96%|\n",
      "|09/01/2024| 63,339.2| 58,975.7| 66,440.7|52,644.6|2.34M|   7.39%|\n",
      "|08/01/2024| 58,978.6| 64,625.7| 65,587.9|49,486.9|2.56M|  -8.74%|\n",
      "|07/01/2024| 64,626.0| 62,768.8| 70,000.2|53,883.4|2.06M|   2.98%|\n",
      "|06/01/2024| 62,754.3| 67,533.9| 71,956.5|58,589.9|1.60M|  -7.07%|\n",
      "|05/01/2024| 67,530.1| 60,665.0| 71,872.0|56,643.5|2.10M|  11.31%|\n",
      "|04/01/2024| 60,666.6| 71,329.3| 72,710.8|59,228.7|2.66M| -14.95%|\n",
      "|03/01/2024| 71,332.0| 61,157.3| 73,740.9|60,138.2|2.70M|  16.61%|\n",
      "|02/01/2024| 61,169.3| 42,580.1| 63,915.3|41,890.5|1.74M|  43.66%|\n",
      "|01/01/2024| 42,580.5| 42,272.5| 48,923.7|38,546.9|2.03M|   0.73%|\n",
      "|12/01/2023| 42,272.5| 37,712.9| 44,697.6|37,618.3|1.63M|  12.09%|\n",
      "|11/01/2023| 37,712.9| 34,648.3| 38,400.8|34,124.2|1.48M|   8.84%|\n",
      "|10/01/2023| 34,650.6| 26,962.5| 35,191.4|26,558.4|1.61M|  28.51%|\n",
      "|09/01/2023| 26,962.7| 25,938.3| 27,480.7|24,923.1|1.15M|   3.95%|\n",
      "+----------+---------+---------+---------+--------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Vol.: string (nullable = true)\n",
      " |-- Change %: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_btc.show(20)\n",
    "df_btc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Bitcoin Dataset\n",
    "\n",
    "This dataset contains **monthly data** for Bitcoin from **August 1, 2010** to **April 1, 2025**.\n",
    "\n",
    "> ⚠️ **Note**: The data for **April 2025** is not final, as this project was created during the same month. Interpret that row with caution.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Information:\n",
    "\n",
    "- Each row represents **a full calendar month**.\n",
    "- The `Date` column uses the **first day of the month** as a label (e.g. `2025-04-01` refers to data from April 2025).\n",
    "- The values shown reflect **the entire month**, not just that specific day.\n",
    "\n",
    "---\n",
    "\n",
    "### Column Descriptions:\n",
    "\n",
    "| Column   | Description                                                |\n",
    "|----------|------------------------------------------------------------|\n",
    "| `Date`   | First day of the month (used as time label)                |\n",
    "| `Open`   | Price at the start of the month                            |\n",
    "| `Price`  | Closing price at the end of the month                      |\n",
    "| `High`   | Maximum price reached during the month                     |\n",
    "| `Low`    | Minimum price reached during the month                     |\n",
    "| `Change` | % change between the opening and closing price of the month|\n",
    "\n",
    "> ℹ️ Later on, I rename the column `Price` to `Close` to make it clearer in context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+--------+-------+\n",
      "|      Date|    Close|     Open|     High|     Low| Change|\n",
      "+----------+---------+---------+---------+--------+-------+\n",
      "|2025-04-01| 84,931.6| 82,548.6| 88,377.3|74,524.2|  2.89%|\n",
      "|2025-03-01| 82,548.8| 84,353.4| 94,986.5|76,677.1| -2.17%|\n",
      "|2025-02-01| 84,381.2|102,421.3|102,770.8|78,329.8|-17.62%|\n",
      "|2025-01-01|102,424.2| 93,557.3|109,228.6|89,664.8|  9.48%|\n",
      "|2024-12-01| 93,557.2| 96,404.7|108,244.9|91,522.3| -2.95%|\n",
      "|2024-11-01| 96,405.7| 70,278.7| 99,617.4|66,834.0| 37.17%|\n",
      "|2024-10-01| 70,281.8| 63,329.9| 73,569.4|59,075.7| 10.96%|\n",
      "|2024-09-01| 63,339.2| 58,975.7| 66,440.7|52,644.6|  7.39%|\n",
      "|2024-08-01| 58,978.6| 64,625.7| 65,587.9|49,486.9| -8.74%|\n",
      "|2024-07-01| 64,626.0| 62,768.8| 70,000.2|53,883.4|  2.98%|\n",
      "|2024-06-01| 62,754.3| 67,533.9| 71,956.5|58,589.9| -7.07%|\n",
      "|2024-05-01| 67,530.1| 60,665.0| 71,872.0|56,643.5| 11.31%|\n",
      "|2024-04-01| 60,666.6| 71,329.3| 72,710.8|59,228.7|-14.95%|\n",
      "|2024-03-01| 71,332.0| 61,157.3| 73,740.9|60,138.2| 16.61%|\n",
      "|2024-02-01| 61,169.3| 42,580.1| 63,915.3|41,890.5| 43.66%|\n",
      "|2024-01-01| 42,580.5| 42,272.5| 48,923.7|38,546.9|  0.73%|\n",
      "|2023-12-01| 42,272.5| 37,712.9| 44,697.6|37,618.3| 12.09%|\n",
      "|2023-11-01| 37,712.9| 34,648.3| 38,400.8|34,124.2|  8.84%|\n",
      "|2023-10-01| 34,650.6| 26,962.5| 35,191.4|26,558.4| 28.51%|\n",
      "|2023-09-01| 26,962.7| 25,938.3| 27,480.7|24,923.1|  3.95%|\n",
      "+----------+---------+---------+---------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Change: string (nullable = true)\n",
      "\n",
      "+------------+------------+\n",
      "|Fecha mínima|Fecha máxima|\n",
      "+------------+------------+\n",
      "|  2010-08-01|  2025-04-01|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_btc = df_btc.withColumn(\"Date\", F.to_date(\"Date\", \"MM/dd/yyyy\"))\n",
    "\n",
    "df_btc = df_btc.select(#F.date_format(\"Date\", \"dd-MM-yyyy\").alias(\"Date\"), Cambia a String y no quiero\n",
    "              F.col(\"Date\"),\n",
    "              F.col(\"Price\").alias(\"Close\"),\n",
    "              F.col(\"Open\"),\n",
    "              F.col(\"High\"),\n",
    "              F.col(\"Low\"),\n",
    "              F.col(\"Change %\").alias(\"Change\")\n",
    "             )\n",
    "df_btc.show()\n",
    "\n",
    "df_btc.printSchema()\n",
    "\n",
    "df_btc.select(\n",
    "    F.min(\"Date\").alias(\"Fecha mínima\"),\n",
    "    F.max(\"Date\").alias(\"Fecha máxima\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = df_btc.withColumn(\n",
    "    \"block_reward\",\n",
    "    F.when(F.col(\"Date\") <= F.lit(\"2012-11-01\"), 50)\n",
    "     .when((F.col(\"Date\") > F.lit(\"2012-11-01\")) & (F.col(\"Date\") <= F.lit(\"2016-06-01\")), 25)\n",
    "     .when((F.col(\"Date\") > F.lit(\"2016-06-01\")) & (F.col(\"Date\") <= F.lit(\"2020-04-01\")), 12.5)\n",
    "     .when((F.col(\"Date\") > F.lit(\"2020-04-01\")) & (F.col(\"Date\") <= F.lit(\"2024-04-01\")), 6.25)\n",
    "     .otherwise(3.125)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change</th>\n",
       "      <th>block_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>84,931.6</td>\n",
       "      <td>82,548.6</td>\n",
       "      <td>88,377.3</td>\n",
       "      <td>74,524.2</td>\n",
       "      <td>2.89%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>82,548.8</td>\n",
       "      <td>84,353.4</td>\n",
       "      <td>94,986.5</td>\n",
       "      <td>76,677.1</td>\n",
       "      <td>-2.17%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>84,381.2</td>\n",
       "      <td>102,421.3</td>\n",
       "      <td>102,770.8</td>\n",
       "      <td>78,329.8</td>\n",
       "      <td>-17.62%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>102,424.2</td>\n",
       "      <td>93,557.3</td>\n",
       "      <td>109,228.6</td>\n",
       "      <td>89,664.8</td>\n",
       "      <td>9.48%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>93,557.2</td>\n",
       "      <td>96,404.7</td>\n",
       "      <td>108,244.9</td>\n",
       "      <td>91,522.3</td>\n",
       "      <td>-2.95%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>96,405.7</td>\n",
       "      <td>70,278.7</td>\n",
       "      <td>99,617.4</td>\n",
       "      <td>66,834.0</td>\n",
       "      <td>37.17%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>70,281.8</td>\n",
       "      <td>63,329.9</td>\n",
       "      <td>73,569.4</td>\n",
       "      <td>59,075.7</td>\n",
       "      <td>10.96%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>63,339.2</td>\n",
       "      <td>58,975.7</td>\n",
       "      <td>66,440.7</td>\n",
       "      <td>52,644.6</td>\n",
       "      <td>7.39%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>58,978.6</td>\n",
       "      <td>64,625.7</td>\n",
       "      <td>65,587.9</td>\n",
       "      <td>49,486.9</td>\n",
       "      <td>-8.74%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>64,626.0</td>\n",
       "      <td>62,768.8</td>\n",
       "      <td>70,000.2</td>\n",
       "      <td>53,883.4</td>\n",
       "      <td>2.98%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>62,754.3</td>\n",
       "      <td>67,533.9</td>\n",
       "      <td>71,956.5</td>\n",
       "      <td>58,589.9</td>\n",
       "      <td>-7.07%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>67,530.1</td>\n",
       "      <td>60,665.0</td>\n",
       "      <td>71,872.0</td>\n",
       "      <td>56,643.5</td>\n",
       "      <td>11.31%</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>60,666.6</td>\n",
       "      <td>71,329.3</td>\n",
       "      <td>72,710.8</td>\n",
       "      <td>59,228.7</td>\n",
       "      <td>-14.95%</td>\n",
       "      <td>6.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Close       Open       High       Low   Change  \\\n",
       "0   2025-04-01   84,931.6   82,548.6   88,377.3  74,524.2    2.89%   \n",
       "1   2025-03-01   82,548.8   84,353.4   94,986.5  76,677.1   -2.17%   \n",
       "2   2025-02-01   84,381.2  102,421.3  102,770.8  78,329.8  -17.62%   \n",
       "3   2025-01-01  102,424.2   93,557.3  109,228.6  89,664.8    9.48%   \n",
       "4   2024-12-01   93,557.2   96,404.7  108,244.9  91,522.3   -2.95%   \n",
       "5   2024-11-01   96,405.7   70,278.7   99,617.4  66,834.0   37.17%   \n",
       "6   2024-10-01   70,281.8   63,329.9   73,569.4  59,075.7   10.96%   \n",
       "7   2024-09-01   63,339.2   58,975.7   66,440.7  52,644.6    7.39%   \n",
       "8   2024-08-01   58,978.6   64,625.7   65,587.9  49,486.9   -8.74%   \n",
       "9   2024-07-01   64,626.0   62,768.8   70,000.2  53,883.4    2.98%   \n",
       "10  2024-06-01   62,754.3   67,533.9   71,956.5  58,589.9   -7.07%   \n",
       "11  2024-05-01   67,530.1   60,665.0   71,872.0  56,643.5   11.31%   \n",
       "12  2024-04-01   60,666.6   71,329.3   72,710.8  59,228.7  -14.95%   \n",
       "\n",
       "    block_reward  \n",
       "0          3.125  \n",
       "1          3.125  \n",
       "2          3.125  \n",
       "3          3.125  \n",
       "4          3.125  \n",
       "5          3.125  \n",
       "6          3.125  \n",
       "7          3.125  \n",
       "8          3.125  \n",
       "9          3.125  \n",
       "10         3.125  \n",
       "11         3.125  \n",
       "12         6.250  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_btc.orderBy(F.col(\"Date\").desc()).toPandas().head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Reward Column\n",
    "\n",
    "To account for Bitcoin’s changing monetary policy, I added a `block_reward` column that reflects the number of BTC miners received per block over time.\n",
    "\n",
    "---\n",
    "\n",
    "### Halving Schedule:\n",
    "\n",
    "| Halving | Date             | Block Reward |\n",
    "|---------|------------------|--------------|\n",
    "| 1st     | Nov 28, 2012     | 25 BTC       |\n",
    "| 2nd     | Jul 9, 2016      | 12.5 BTC     |\n",
    "| 3rd     | May 11, 2020     | 6.25 BTC     |\n",
    "| 4th     | Apr 20, 2024     | 3.125 BTC    |\n",
    "\n",
    "---\n",
    "\n",
    "### Logic:\n",
    "\n",
    "- My dataset is monthly (each row = 1st day of the month).\n",
    "- Since the 2012 and 2024 halvings occurred at the **end of the month**, I assigned those entire months (`2012-11` and `2024-04`) to the **previous reward**.\n",
    "- This better reflects the fact that most of the blocks mined during those months still followed the old reward.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Period                 | Assigned Block Reward |\n",
    "|------------------------|------------------------|\n",
    "| ≤ November 2012        | 50 BTC                 |\n",
    "| December 2012 – June 2016 | 25 BTC             |\n",
    "| July 2016 – April 2020    | 12.5 BTC            |\n",
    "| May 2020 – April 2024     | 6.25 BTC            |\n",
    "| ≥ May 2024               | 3.125 BTC            |\n",
    "\n",
    "This column will help in analyzing how price and volatility behave across halving cycles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Change: string (nullable = true)\n",
      " |-- block_reward: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_btc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Change: double (nullable = true)\n",
      " |-- block_reward: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_convert = [\"Close\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_btc = df_btc.withColumn(col, F.regexp_replace(F.col(col), \",\", \"\").cast(DoubleType()))\n",
    "\n",
    "df_btc = df_btc.withColumn(\n",
    "    \"Change\",\n",
    "    F.regexp_replace(F.col(\"Change\"), \"%\", \"\").cast(DoubleType())\n",
    ")\n",
    "df_btc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+-------+------+------------+\n",
      "|      Date|   Close|    Open|    High|    Low|Change|block_reward|\n",
      "+----------+--------+--------+--------+-------+------+------------+\n",
      "|2025-04-01| 84931.6| 82548.6| 88377.3|74524.2|  2.89|       3.125|\n",
      "|2025-03-01| 82548.8| 84353.4| 94986.5|76677.1| -2.17|       3.125|\n",
      "|2025-02-01| 84381.2|102421.3|102770.8|78329.8|-17.62|       3.125|\n",
      "|2025-01-01|102424.2| 93557.3|109228.6|89664.8|  9.48|       3.125|\n",
      "|2024-12-01| 93557.2| 96404.7|108244.9|91522.3| -2.95|       3.125|\n",
      "|2024-11-01| 96405.7| 70278.7| 99617.4|66834.0| 37.17|       3.125|\n",
      "|2024-10-01| 70281.8| 63329.9| 73569.4|59075.7| 10.96|       3.125|\n",
      "|2024-09-01| 63339.2| 58975.7| 66440.7|52644.6|  7.39|       3.125|\n",
      "|2024-08-01| 58978.6| 64625.7| 65587.9|49486.9| -8.74|       3.125|\n",
      "|2024-07-01| 64626.0| 62768.8| 70000.2|53883.4|  2.98|       3.125|\n",
      "+----------+--------+--------+--------+-------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_btc.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv('../data/final_df_extended.csv')\n",
    "\n",
    "df_extended = df_extended.withColumn(\"Date\", \n",
    "    F.to_date(F.concat(F.col(\"month\"), F.lit(\"-01\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_extended = df_extended.drop(\"month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o565.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 45) (DESKTOP-1JVP2OE executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-09-01 00:00:00-01' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.time.format.DateTimeParseException: Text '2014-09-01 00:00:00-01' could not be parsed, unparsed text found at index 10\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-09-01 00:00:00-01' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '2014-09-01 00:00:00-01' could not be parsed, unparsed text found at index 10\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\btc_portfolio\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\btc_portfolio\\lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\btc_portfolio\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\btc_portfolio\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\btc_portfolio\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o565.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 45) (DESKTOP-1JVP2OE executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-09-01 00:00:00-01' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.time.format.DateTimeParseException: Text '2014-09-01 00:00:00-01' could not be parsed, unparsed text found at index 10\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-09-01 00:00:00-01' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '2014-09-01 00:00:00-01' could not be parsed, unparsed text found at index 10\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "df_extended.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>halving_date</th>\n",
       "      <th>block_reward</th>\n",
       "      <th>months_after</th>\n",
       "      <th>return_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>1</td>\n",
       "      <td>7.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>3</td>\n",
       "      <td>165.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>6</td>\n",
       "      <td>922.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>9</td>\n",
       "      <td>1019.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>12</td>\n",
       "      <td>9469.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>15</td>\n",
       "      <td>4454.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>18</td>\n",
       "      <td>4883.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>21</td>\n",
       "      <td>3723.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24</td>\n",
       "      <td>2875.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>27</td>\n",
       "      <td>1916.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>30</td>\n",
       "      <td>1723.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>38</td>\n",
       "      <td>2834.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>25.000</td>\n",
       "      <td>46</td>\n",
       "      <td>4726.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.718283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>3</td>\n",
       "      <td>12.349252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>6</td>\n",
       "      <td>55.250040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>9</td>\n",
       "      <td>117.382216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>12</td>\n",
       "      <td>363.627593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>15</td>\n",
       "      <td>937.337192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>18</td>\n",
       "      <td>1550.651230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>21</td>\n",
       "      <td>1386.589484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>24</td>\n",
       "      <td>1142.868628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>27</td>\n",
       "      <td>923.621161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>30</td>\n",
       "      <td>452.693359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>38</td>\n",
       "      <td>1232.095192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>12.500</td>\n",
       "      <td>46</td>\n",
       "      <td>1420.308731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.378178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>3</td>\n",
       "      <td>23.156492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>6</td>\n",
       "      <td>108.339679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>9</td>\n",
       "      <td>377.683293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>12</td>\n",
       "      <td>294.493802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>15</td>\n",
       "      <td>398.481195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>18</td>\n",
       "      <td>501.629860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>21</td>\n",
       "      <td>356.785971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>24</td>\n",
       "      <td>236.267293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>27</td>\n",
       "      <td>111.997081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>30</td>\n",
       "      <td>81.536362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>38</td>\n",
       "      <td>209.180522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>6.250</td>\n",
       "      <td>46</td>\n",
       "      <td>654.452765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>1</td>\n",
       "      <td>11.313474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>3</td>\n",
       "      <td>6.526491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>6</td>\n",
       "      <td>15.849248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>9</td>\n",
       "      <td>68.831284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>12</td>\n",
       "      <td>39.997297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3.125</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   halving_date  block_reward  months_after     return_%\n",
       "0    2012-11-01        25.000             1     7.142857\n",
       "1    2012-11-01        25.000             3   165.079365\n",
       "2    2012-11-01        25.000             6   922.222222\n",
       "3    2012-11-01        25.000             9  1019.047619\n",
       "4    2012-11-01        25.000            12  9469.047619\n",
       "5    2012-11-01        25.000            15  4454.761905\n",
       "6    2012-11-01        25.000            18  4883.333333\n",
       "7    2012-11-01        25.000            21  3723.809524\n",
       "8    2012-11-01        25.000            24  2875.396825\n",
       "9    2012-11-01        25.000            27  1916.666667\n",
       "10   2012-11-01        25.000            30  1723.809524\n",
       "11   2012-11-01        25.000            38  2834.920635\n",
       "12   2012-11-01        25.000            46  4726.190476\n",
       "13   2016-07-01        12.500             1    -7.718283\n",
       "14   2016-07-01        12.500             3    12.349252\n",
       "15   2016-07-01        12.500             6    55.250040\n",
       "16   2016-07-01        12.500             9   117.382216\n",
       "17   2016-07-01        12.500            12   363.627593\n",
       "18   2016-07-01        12.500            15   937.337192\n",
       "19   2016-07-01        12.500            18  1550.651230\n",
       "20   2016-07-01        12.500            21  1386.589484\n",
       "21   2016-07-01        12.500            24  1142.868628\n",
       "22   2016-07-01        12.500            27   923.621161\n",
       "23   2016-07-01        12.500            30   452.693359\n",
       "24   2016-07-01        12.500            38  1232.095192\n",
       "25   2016-07-01        12.500            46  1420.308731\n",
       "26   2020-05-01         6.250             1    -3.378178\n",
       "27   2020-05-01         6.250             3    23.156492\n",
       "28   2020-05-01         6.250             6   108.339679\n",
       "29   2020-05-01         6.250             9   377.683293\n",
       "30   2020-05-01         6.250            12   294.493802\n",
       "31   2020-05-01         6.250            15   398.481195\n",
       "32   2020-05-01         6.250            18   501.629860\n",
       "33   2020-05-01         6.250            21   356.785971\n",
       "34   2020-05-01         6.250            24   236.267293\n",
       "35   2020-05-01         6.250            27   111.997081\n",
       "36   2020-05-01         6.250            30    81.536362\n",
       "37   2020-05-01         6.250            38   209.180522\n",
       "38   2020-05-01         6.250            46   654.452765\n",
       "39   2024-04-01         3.125             1    11.313474\n",
       "40   2024-04-01         3.125             3     6.526491\n",
       "41   2024-04-01         3.125             6    15.849248\n",
       "42   2024-04-01         3.125             9    68.831284\n",
       "43   2024-04-01         3.125            12    39.997297\n",
       "44   2024-04-01         3.125            15          NaN\n",
       "45   2024-04-01         3.125            18          NaN\n",
       "46   2024-04-01         3.125            21          NaN\n",
       "47   2024-04-01         3.125            24          NaN\n",
       "48   2024-04-01         3.125            27          NaN\n",
       "49   2024-04-01         3.125            30          NaN\n",
       "50   2024-04-01         3.125            38          NaN\n",
       "51   2024-04-01         3.125            46          NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Fechas de los halvings\n",
    "halvings = {\n",
    "    \"2012-11-01\": 25,\n",
    "    \"2016-07-01\": 12.5,\n",
    "    \"2020-05-01\": 6.25,\n",
    "    \"2024-04-01\": 3.125\n",
    "}\n",
    "\n",
    "# Ventanas que quieres analizar\n",
    "months_after = [1, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 38, 46]\n",
    "\n",
    "results = []\n",
    "\n",
    "for halving_date, reward in halvings.items():\n",
    "    halving_close = df_btc.filter(F.col(\"Date\") == halving_date).select(\"Close\").collect()[0][0]\n",
    "\n",
    "    for m in months_after:\n",
    "        target_date = pd.to_datetime(halving_date) + pd.DateOffset(months=m)\n",
    "        target_date = target_date.strftime(\"%Y-%m-01\")  # formatear como string yyyy-MM-01\n",
    "\n",
    "        df_target = df_btc.filter(F.col(\"Date\") == F.lit(target_date))\n",
    "\n",
    "        if df_target.count() > 0:\n",
    "            target_close = df_target.select(\"Close\").collect()[0][0]\n",
    "            ret = (target_close / halving_close - 1) * 100\n",
    "            results.append([halving_date, reward, m, ret])\n",
    "        else:\n",
    "            results.append([halving_date, reward, m, None])  # No hay data todavía\n",
    "\n",
    "# Crear pandas dataframe\n",
    "df_performance = pd.DataFrame(results, columns=[\"halving_date\", \"block_reward\", \"months_after\", \"return_%\"])\n",
    "\n",
    "display(df_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_after</th>\n",
       "      <th>return_%</th>\n",
       "      <th>projected_price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>60387.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>62577.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>14.72</td>\n",
       "      <td>69596.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>51.30</td>\n",
       "      <td>91788.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>40.00</td>\n",
       "      <td>84933.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>54.12</td>\n",
       "      <td>93499.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>68.14</td>\n",
       "      <td>102004.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>48.46</td>\n",
       "      <td>90065.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>32.09</td>\n",
       "      <td>80134.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>15.21</td>\n",
       "      <td>69893.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>11.08</td>\n",
       "      <td>67388.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>38</td>\n",
       "      <td>28.41</td>\n",
       "      <td>77901.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>46</td>\n",
       "      <td>88.89</td>\n",
       "      <td>114593.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    months_after  return_%  projected_price_usd\n",
       "0              1     -0.46             60387.53\n",
       "1              3      3.15             62577.60\n",
       "2              6     14.72             69596.72\n",
       "3              9     51.30             91788.57\n",
       "4             12     40.00             84933.24\n",
       "5             15     54.12             93499.36\n",
       "6             18     68.14            102004.82\n",
       "7             21     48.46             90065.63\n",
       "8             24     32.09             80134.51\n",
       "9             27     15.21             69893.99\n",
       "10            30     11.08             67388.46\n",
       "11            38     28.41             77901.98\n",
       "12            46     88.89            114593.14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Precio base (Close justo antes del halving - marzo 2024)\n",
    "precio_base = 60666.60\n",
    "\n",
    "# Ratio de reducción de retornos entre ciclo 2020 y 2024\n",
    "reduction_ratio = 294.49 / 40.00  # ≈ 7.36\n",
    "\n",
    "# Retornos reales ciclo 2020 desde mes 1 hasta 46\n",
    "returns_2020 = {\n",
    "    1: -3.38,\n",
    "    3: 23.16,\n",
    "    6: 108.34,\n",
    "    9: 377.68,\n",
    "    12: 294.49,\n",
    "    15: 398.48,\n",
    "    18: 501.63,\n",
    "    21: 356.79,\n",
    "    24: 236.27,\n",
    "    27: 111.99,\n",
    "    30: 81.54,\n",
    "    38: 209.18,\n",
    "    46: 654.45\n",
    "}\n",
    "\n",
    "# Aplicar reducción para predecir retornos 2024\n",
    "retornos_2024 = {k: round(v / reduction_ratio, 2) for k, v in returns_2020.items()}\n",
    "\n",
    "# Calcular precio final estimado\n",
    "resultados = []\n",
    "for meses, retorno in retornos_2024.items():\n",
    "    precio_estimado = round(precio_base * (1 + retorno / 100), 2)\n",
    "    resultados.append({\n",
    "        'months_after': meses,\n",
    "        'return_%': retorno,\n",
    "        'projected_price_usd': precio_estimado\n",
    "    })\n",
    "\n",
    "# Crear DataFrame\n",
    "df_prediccion_2024 = pd.DataFrame(resultados).sort_values('months_after').reset_index(drop=True)\n",
    "\n",
    "# Mostrar tabla final\n",
    "display(df_prediccion_2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
